{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b025b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def unpickle(fn):\n",
    "    with open(fn, 'rb') as fo:\n",
    "        data = np.load(fn, allow_pickle=True)\n",
    "    return data['x'], data['y']\n",
    "    #return x, y\n",
    "\n",
    "import sys, pathlib, random\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "import shutil\n",
    "\n",
    "pathlib.Path('data').mkdir(exist_ok=True)\n",
    "datadir_recycle = pathlib.Path('data/recycle')\n",
    "datadir_recycle_small = pathlib.Path('data/recycle-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9b4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_recycle_data(npz_filename):\n",
    "    #data from http://web.cecs.pdx.edu/~singh/rcyc-web/dataset.html\n",
    "    d = np.load(npz_filename, allow_pickle=True)\n",
    "    datadir = datadir_recycle\n",
    "    datadir_small = datadir_recycle_small\n",
    "\n",
    "    if datadir.exists() and datadir_small.exists():\n",
    "        print(f'{datadir} and {datadir_small} already exist, skipping download of recycle dataset')\n",
    "        return\n",
    "    \n",
    "    if datadir.exists(): shutil.rmtree(datadir)\n",
    "    if datadir_small.exists(): shutil.rmtree(datadir_small)\n",
    "    \n",
    "    labels = ['boxes', 'glass_bottles', 'soda_cans', 'crushed_soda_cans', 'water_bottles']\n",
    "   \n",
    "    datadir.mkdir(exist_ok=True)\n",
    "    datadir_small.mkdir(exist_ok=True)\n",
    "    for dirn in ['train', 'val', 'test']:\n",
    "        (datadir / dirn).mkdir(exist_ok=True)\n",
    "        (datadir_small / dirn).mkdir(exist_ok=True)\n",
    "        for label in labels:\n",
    "            (datadir/ dirn / str(label)).mkdir(exist_ok=True)\n",
    "            (datadir_small / dirn / str(label)).mkdir(exist_ok=True)\n",
    "\n",
    "    print(d.files)\n",
    "    print('splitting training set')\n",
    "    for i,x in enumerate(d['x_train']):\n",
    "        dirn = 'train' if random.random() > 0.2 else 'val'\n",
    "        y = d['y_train'][i][0]\n",
    "        if i%100 == 0: print(i,y, label, dirn)\n",
    "\n",
    "        label = labels[y]\n",
    "        img = Image.fromarray(x)\n",
    "        img.save(datadir / dirn / label / f'{i}.jpg')\n",
    "        if random.random() < 0.22:\n",
    "            img.save(datadir_small / dirn / label / f'{i}.jpg')\n",
    "\n",
    "    print('saving test set')\n",
    "    for i,x in enumerate(d['x_test']):\n",
    "        dirn = 'test'\n",
    "        y = d['y_test'][i][0]\n",
    "        if i%100 == 0: print(i,y, label, dirn)\n",
    "\n",
    "        label = labels[y]\n",
    "        img = Image.fromarray(x)\n",
    "        img.save(datadir / dirn / label / f'{i}.jpg')\n",
    "\n",
    "        if random.random() < 0.22:\n",
    "            img.save(datadir_small / dirn / label / f'{i}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8536f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading http://web.cecs.pdx.edu/~singh/rcyc-web/recycle_data_shuffled.tar.gz....\n",
      "done\n",
      "extracting data from data/recycle_data_shuffled.tar.gz\n",
      "['x_train', 'y_train', 'x_test', 'y_test']\n",
      "splitting training set\n",
      "0 0 water_bottles val\n",
      "100 2 boxes train\n",
      "200 3 boxes train\n",
      "300 0 soda_cans val\n",
      "400 4 crushed_soda_cans val\n",
      "500 0 glass_bottles train\n",
      "600 0 crushed_soda_cans train\n",
      "700 1 glass_bottles val\n",
      "800 3 boxes train\n",
      "900 2 water_bottles train\n",
      "1000 4 crushed_soda_cans train\n",
      "1100 0 crushed_soda_cans val\n",
      "1200 3 soda_cans train\n",
      "1300 4 soda_cans train\n",
      "1400 2 soda_cans train\n",
      "1500 0 crushed_soda_cans train\n",
      "1600 1 crushed_soda_cans train\n",
      "1700 4 glass_bottles train\n",
      "1800 3 water_bottles train\n",
      "1900 4 crushed_soda_cans train\n",
      "2000 3 water_bottles train\n",
      "2100 1 soda_cans val\n",
      "2200 2 boxes train\n",
      "2300 1 glass_bottles train\n",
      "2400 1 soda_cans train\n",
      "2500 3 soda_cans train\n",
      "2600 3 glass_bottles train\n",
      "2700 1 boxes val\n",
      "2800 1 boxes val\n",
      "2900 2 boxes train\n",
      "3000 4 soda_cans val\n",
      "3100 2 soda_cans val\n",
      "3200 4 water_bottles train\n",
      "3300 0 glass_bottles train\n",
      "3400 1 soda_cans val\n",
      "3500 1 soda_cans train\n",
      "3600 2 boxes train\n",
      "3700 2 glass_bottles train\n",
      "3800 3 boxes train\n",
      "3900 1 crushed_soda_cans train\n",
      "4000 3 glass_bottles train\n",
      "4100 2 soda_cans train\n",
      "4200 3 water_bottles train\n",
      "4300 3 water_bottles train\n",
      "4400 1 soda_cans train\n",
      "4500 4 glass_bottles train\n",
      "4600 0 boxes train\n",
      "4700 3 soda_cans train\n",
      "4800 2 soda_cans val\n",
      "4900 2 soda_cans train\n",
      "5000 2 soda_cans train\n",
      "5100 1 crushed_soda_cans train\n",
      "5200 3 water_bottles val\n",
      "5300 4 water_bottles val\n",
      "5400 1 soda_cans train\n",
      "5500 0 soda_cans val\n",
      "5600 4 glass_bottles train\n",
      "5700 0 crushed_soda_cans val\n",
      "5800 2 soda_cans val\n",
      "5900 3 glass_bottles val\n",
      "6000 4 soda_cans train\n",
      "6100 0 crushed_soda_cans train\n",
      "6200 1 glass_bottles val\n",
      "6300 0 crushed_soda_cans train\n",
      "6400 3 boxes train\n",
      "6500 3 boxes val\n",
      "6600 3 crushed_soda_cans train\n",
      "6700 3 crushed_soda_cans train\n",
      "6800 0 glass_bottles train\n",
      "6900 1 crushed_soda_cans train\n",
      "7000 2 boxes train\n",
      "7100 2 glass_bottles train\n",
      "7200 4 water_bottles val\n",
      "7300 3 water_bottles train\n",
      "7400 0 glass_bottles train\n",
      "7500 3 water_bottles train\n",
      "7600 0 boxes val\n",
      "7700 1 crushed_soda_cans val\n",
      "7800 2 glass_bottles train\n",
      "7900 3 soda_cans train\n",
      "8000 2 crushed_soda_cans train\n",
      "8100 0 crushed_soda_cans train\n",
      "8200 4 water_bottles train\n",
      "8300 2 water_bottles train\n",
      "8400 2 crushed_soda_cans train\n",
      "8500 3 crushed_soda_cans val\n",
      "8600 3 soda_cans train\n",
      "8700 1 crushed_soda_cans train\n",
      "8800 1 soda_cans train\n",
      "8900 2 glass_bottles train\n",
      "9000 4 soda_cans train\n",
      "9100 0 crushed_soda_cans train\n",
      "saving test set\n",
      "0 1 soda_cans test\n",
      "100 0 soda_cans test\n",
      "200 2 water_bottles test\n",
      "300 3 water_bottles test\n",
      "400 0 boxes test\n",
      "500 4 crushed_soda_cans test\n",
      "600 1 glass_bottles test\n",
      "700 3 boxes test\n",
      "800 4 water_bottles test\n",
      "900 3 boxes test\n",
      "1000 1 boxes test\n",
      "1100 0 boxes test\n",
      "1200 4 soda_cans test\n",
      "1300 1 soda_cans test\n",
      "1400 2 crushed_soda_cans test\n",
      "1500 0 soda_cans test\n",
      "1600 0 boxes test\n",
      "1700 2 glass_bottles test\n",
      "1800 2 soda_cans test\n",
      "1900 3 water_bottles test\n",
      "2000 4 soda_cans test\n",
      "2100 0 boxes test\n",
      "2200 4 glass_bottles test\n",
      "cleaning up data/recycle_data_shuffled.tar.gz\n",
      "cleaning up data/recycle_data_shuffled.npz\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "url = 'http://web.cecs.pdx.edu/~singh/rcyc-web/recycle_data_shuffled.tar.gz'\n",
    "tarballfn = pathlib.Path('data/recycle_data_shuffled.tar.gz')\n",
    "npzfn = pathlib.Path('data/recycle_data_shuffled.npz')\n",
    "\n",
    "if datadir_recycle.exists() and datadir_recycle_small.exists():\n",
    "    print(f'{datadir_recycle} and {datadir_recycle_small} already exist, skipping download of recycle dataset')\n",
    "else:\n",
    "    if not npzfn.exists():\n",
    "        if not tarballfn.exists():\n",
    "            print(f'downloading {url}....')\n",
    "            urllib.request.urlretrieve(url, tarballfn)\n",
    "            print('done')\n",
    "        else:\n",
    "            print(f'{tarballfn} exists, skipping download')\n",
    "\n",
    "        print(f'extracting data from {tarballfn}')\n",
    "        tar = tarfile.open(tarballfn, \"r:gz\")\n",
    "        tar.extractall(path='data')\n",
    "        tar.close()\n",
    "        split_recycle_data(npzfn)\n",
    "    else:\n",
    "        print(f'{npzfn} exists, skipping tarball download')\n",
    "\n",
    "if tarballfn.exists():\n",
    "    print(f'cleaning up {tarballfn}')\n",
    "    tarballfn.unlink()\n",
    "\n",
    "if npzfn.exists():\n",
    "    print(f'cleaning up {npzfn}')\n",
    "    npzfn.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08b776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets --upgrade --quiet\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cb8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: gordonhog\n",
      "Your Kaggle Key: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/82.0M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading garbage-classification.zip to ./garbage-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82.0M/82.0M [00:11<00:00, 7.35MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing garbage-classification/Garbage classification/Garbage classification/plastic\n",
      "doing garbage-classification/Garbage classification/Garbage classification/cardboard\n",
      "doing garbage-classification/Garbage classification/Garbage classification/paper\n",
      "doing garbage-classification/Garbage classification/Garbage classification/metal\n",
      "doing garbage-classification/Garbage classification/Garbage classification/glass\n",
      "doing garbage-classification/Garbage classification/Garbage classification/trash\n"
     ]
    }
   ],
   "source": [
    "origdir = pathlib.Path('./garbage-classification/Garbage classification/Garbage classification')\n",
    "targetdir = pathlib.Path('data/garbage')\n",
    "\n",
    "if targetdir.exists():\n",
    "    print(f'{targetdir} exists, skipping download and prep of garbage dataset')\n",
    "else:\n",
    "    od.download('https://www.kaggle.com/asdasdasasdas/garbage-classification')\n",
    "\n",
    "    targetdir.mkdir()\n",
    "\n",
    "    for setn in ['train','test', 'val']:\n",
    "       (targetdir / setn).mkdir()\n",
    "    #20% for test\n",
    "    #then 20% of the remaining for val\n",
    "\n",
    "    for dirn in origdir.iterdir():\n",
    "        print(f'doing {dirn}')\n",
    "\n",
    "        for setn in ['train','test', 'val']:\n",
    "            (targetdir / setn / dirn.name).mkdir()\n",
    "        for fn in dirn.iterdir():\n",
    "            rnd = random.random()\n",
    "            if rnd < 0.2:\n",
    "                #move it to test\n",
    "                setn = 'test'\n",
    "            elif rnd < 0.36: # 20% of 80%\n",
    "                #move it to val\n",
    "                setn = 'val'\n",
    "            else:\n",
    "                #move it to test\n",
    "                setn = 'train'\n",
    "            fn.rename(targetdir / setn / dirn.name / fn.name)\n",
    "\n",
    "    shutil.rmtree('./garbage-classification')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ca9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/inception-garbage-7.model\n",
      "histories/inception-garbage-7.history\n",
      "models/inception-recycle-7.model\n",
      "histories/inception-recycle-7.history\n",
      "models/inception-recycle-small-7.model\n",
      "histories/inception-recycle-small-7.history\n",
      "models/resnet-garbage-3.model\n",
      "histories/resnet-garbage-3.history\n",
      "models/resnet-recycle-3.model\n",
      "histories/resnet-recycle-3.history\n",
      "models/resnet-recycle-small-3.model\n",
      "histories/resnet-recycle-small-3.history\n"
     ]
    }
   ],
   "source": [
    "#now download the pre-trained models and histories\n",
    "for filest in ['inception-garbage-7', 'inception-recycle-7', 'inception-recycle-small-7', 'resnet-garbage-3', 'resnet-recycle-3', 'resnet-recycle-small-3']:\n",
    "    for filep in [pathlib.Path(f'models/{filest}.model'), pathlib.Path(f'histories/{filest}.history')]:\n",
    "        print(filep)\n",
    "    \n",
    "        if not filep.exists():\n",
    "            urllib.request.urlretrieve(f'https://recycle-classifier-models.s3.eu-west-2.amazonaws.com/{filep}', filep)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfff94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
